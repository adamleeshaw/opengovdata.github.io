# Blog Post – Week 3

## Prioritizing Valuable Data

I am reminded when I read the Open Data Directive and the DATA Act of another similarly named initiative, the Paperwork Reduction Act, that ironically adds a few hundred words to just about every lengthy form required by government agencies today.  Which raises the question -- how do we judge the initiatives of government in putting data out there?  What are the benchmarks that we should consider?  

Another observation that raises a similar point.  Just yesterday on John Oliver's weekly comedic rant, the HBO host pointed out that data about pharmaceutical company payments to doctors was posted at [http://openpaymentsdata.cms.gov/](http://openpaymentsdata.cms.gov/).  I pulled up the site and searched my doctor, and found that the site was slow, stalling, and read out error messages for two out of my three refresh attempts.  The data was interesting and might even be seen as useful, but if it is hard to access or under-developed, the utility fades away.

There is no question that open access to information is important in today's Internet age.  But the actual data available and the systems put in place make a difference.  Otherwise, the White House directive and similar initiatives become meaningless.  

The standards that the Sunlight Foundation and Tauberer describe are important first steps, but these benchmarks are for now hidden on niche websites.  The government should adopt these ideas, develop methods of grading open data efforts, and work to develop codified standards that speak to the utility of data, not merely the technical accessibility.  

In particular, we should consider a Freedom of Information Act-like data standard, where citizens and journalists can request that certain types of data be made available, and if the government collects the information, it should be released to the public.  Over time, this type of open access request system could help government prioritize the data that is valuable for open access.

At the same time, government should require minimum standards for websites to function.  Obviously, after the healthcare.gov fiasco, we know that running a government website is difficult.  But these sites should be built to withstand many thousands of requests.  APIs can of course help, especially with third party interest in building off of established government sites.

## Innovation versus Reliability in Legal Databases

The ABA article brings up an important question, and one that proves difficult to answer.  Already, I have found instances where Lexis Nexis does not yield the same results as WestLaw, and that’s even more true with LII and other non-legal databases.  The trouble is that it is difficult to imagine a set of standards that can correct this issue.  It is impossible as it is to ensure reliability on the web, and the expense of reliability is real.  

I see this issue as part of the greater change in the legal profession from a defined club to a more open profession competing with the likes of Legal Zoom and new models of legal advice.  Competition in this respect will always be healthy, and greater access to information is just one more step in that progression.  But at the end of the day, hopefully legal professionals can offer more than the basic law.  They bring reliability and judgment (and maybe even a subscription to WestLaw).  To me, this result makes sense -- the reliable professionals and their databases coexist with the less reliable but still robust legal tools for do-it-yourselfers.  Time will tell where exactly the equilibrium will fall.

## Aside: Archiving the Net

Last week, I wrote that there is an alternative narrative that might unfold to the big data reality:  “The alternative is that with so much data out there, the data returns to its original state as obscure because no person could ever access or sift through the amount of data to make sense of it. Search engines solve this problem today, but if the Internet continues on its path of doubling every year, there may come a point where finding relevant search results is no longer easy.”

I wanted to admit that there is another alternative possible.  The New Yorker recently published the excellent article, “[The Cobweb: What the Web Said Yesterday](http://www.newyorker.com/magazine/2015/01/26/cobweb),” by Jill Lepore.  The article describes the changes to the Internet that have taken place since its inception, and how the Internet does not include a versioning system.  Archival websites such as archive.org have taken responsibility for protecting the integrity of history, but all too often, information can disappear.  In the UK, the Conservative Party removed all mention of a speech given by David Cameron some years back.  Thanks to a government archiving law, that speech was saved, but the new reality may be that despite the growth in information, some information could be lost forever.

This is a problem that we should consider as we think about open government data -- and not just what would happen if tomorrow GitHub’s servers were hacked and the whole site went offline.  But what would happen if government deleted data from years back or purged its information without archive.  What will happen in the year 2100 when someone feels the need to find data from the year 2000.  Will it be available?  Should it be available?  And if it is not, what does that say about history?  Because at the end of the day, every website can be removed from history just as easily as it was created.  For an example more personal to me, check out my high school newspaper’s website (that I just so happened to have created) a mere 7 years since my graduation:  [http://denebolaonline.net/](http://denebolaonline.net/).
